<!DOCTYPE html>
<html lang="en">
	<head>
	    <script src="https://code.jquery.com/jquery-1.11.0.min.js"></script>
		<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
	    <script src="https://netdna.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

	    <!-- Global site tag (gtag.js) - Google Analytics -->
	    <!-- remove this to take out analytics for my website -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-HK5ZED8QD2"></script>
		<script>
			window.dataLayer = window.dataLayer || [];
			function gtag(){dataLayer.push(arguments);}
			gtag('js', new Date());

			gtag('config', 'G-HK5ZED8QD2');
		</script>

		<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css">
		<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.css" rel="stylesheet"  type='text/css'>
		<link rel="stylesheet" href="css/style.css">

		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<title>Millicent Li</title>
	</head>

	<body>
 		<div class="container mt-5">
		  <div class="about row mb-3 mx-auto">
		    <div class="col" align="left">
		    	<h1 class="name">Millicent Li</h1>
		    </div>

		  </div>
		  <div class="row mb-5 mx-auto">
		  	<div class="col-lg-3 col-md-6" align="center">
		    	<img src="img/millicent_li.jpg" alt="Millicent Li" class="img-responsive rounded mb-2" width = "100%">
		    	<div class="row email">
		    		<div class="col">
		    			 <i class="envelope fa fa-envelope fa-lg"></i> <span>li.mil at northeastern dot edu</span> 
		    		</div>
		    	</div>
		    	<div class="row icon">
			    	<div class="col">
			    		<a href="https://scholar.google.com/citations?user=UZ1gBvAAAAAJ&hl=en">
			    			<i class="fa fa-graduation-cap fa-lg" aria-hidden="true"></i>
						</a>
			    		<a href="https://www.linkedin.com/in/limillicent/">
			    			<i class="fa fa-linkedin fa-lg" aria-hidden="true"></i>
						</a>
			    		<a href="https://github.com/millicentli">
			    			<i class="fa fa-github fa-lg" aria-hidden="true"></i>
						</a>
			    		<a href="https://twitter.com/millicent_li">
			    			<i class="fa fa-twitter-square fa-lg" aria-hidden="true"></i>
						</a>
			    	</div>
		    	</div>
		    </div>
		    <div class="col">
		    	<p>I'm a PhD student at <a href="https://www.northeastern.edu/">Northeastern University</a> where I'm advised by <a href="http://www.byronwallace.com/">Byron Wallace</a>. Broadly, I'm interested in investigating language model behaviors. More specifically, language models can precisely generalize on unseen data or pick up on behaviors even though we don't explicitly teach them these behaviors. Where do these abilities come from, and why are models predisposed to pick up on these cues? To this end, I use various tools, in training dynamics and interpretability, to measure and reason about how these behaviors arise. I'm grateful to be supported by a Khoury PhD Fellowship and the <a href="https://www.nsfgrfp.org/">NSF GRFP</a>.</p>

				<p>Before my PhD, I spent time at <a href="https://ai.facebook.com/research/">FAIR/Meta AI</a> as an AI Resident, working with <a href="https://ai.meta.com/people/711449320883014/marjan-ghazvininejad/">Marjan Ghazvininejad</a> and <a href="https://ai.meta.com/people/209431298931133/mike-lewis/">Mike Lewis</a>, and at <a href="https://www.microsoft.com/en-us/research/">Microsoft Research</a>, working with <a href="https://www.microsoft.com/en-us/research/people/tristan/">Tristan Naumann</a>. And even before this, I was an undergrad at the <a href="https://www.washington.edu/">University of Washington</a> working with <a href="https://homes.cs.washington.edu/~shwetak/">Shwetak Patel</a> on ubiquitous computing and <a href="https://homes.cs.washington.edu/~nasmith/">Noah Smith</a> on natural language processing.</p>


		    	<div class="row extras">
		    		<div class="col">
		    			Links:
		    			<a href="files/MillicentLiCV.pdf" target="_blank">CV</a>
		    			<!-- | -->
		    		<!-- </div> -->
		    		<!-- <div class="col"> -->
		    			<!-- <a href="personal.html">Personal</a> -->
		    			<!-- | -->
		    		<!-- </div> -->
		    		<!-- <div class="col"> -->
		    			<!-- <a href="places.html">Places</a> -->
		    			<!-- | -->
		    			<!-- <a href="photography.html">Photography</a> -->
		    		</div>
		    	</div>
		    </div>
		  </div>

		  <!-- Add a divider -->
		  <hr>
		  <!-- Add a divider -->

		  <!-- This is for the news -->
		  <!-- To add a new category, copy paste a row chunk -->
		  <div class="news row mb-3 mx-auto">
		  	<div class="col">
		  		<h2 class="news">News</h2>
		  	</div>
		  </div>
		  <!-- Newest category -->
  		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		January 2025
		  	</div>
		  	<div class="col">

		  		<p>
		  			Our paper, <a href="https://arxiv.org/abs/2410.20056">Multi-Field Adaptive Retrieval</a>, done during my internship at Microsoft Semantic Machines, was accepted to ICLR 2025 as a spotlight (top 5%). Thanks to my amazing co-authors!
		  		</p>
		  	</div>
		  </div>
  		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		October 2024
		  	</div>

		  	<div class="col">
		  		<p>
		  			New preprint on our paper, <a href="https://arxiv.org/abs/2410.20056">Multi-Field Adaptive Retrieval</a>, done during my internship at Microsoft Semantic Machines! 
		  		</p>
		  	</div>
		  </div>
  		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		August 2024
		  	</div>

		  	<div class="col">
		  		<p>
		  			We've released a new preprint on causal interpretability, <a href="https://arxiv.org/abs/2408.01416">The Quest for the Right Mediator: A History, Survey, and Theoretical Grounding of Causal Interpretability</a> - work done with <a href="https://baulab.info/">David Bau's</a> interpretability group.
		  		</p>
		  	</div>
		  </div>
		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		February 2024
		  	</div>

		  	<div class="col">
		  		<p>
		  			I've accepted an internship offer with <a href="https://www.microsoft.com/en-us/research/group/semantic-machines/">Microsoft Semantic Machines</a> for the upcoming summer, working with <a href="https://patrickxia.me/">Patrick Xia</a> and <a href="https://tongfei.me/">Tongfei Chen</a>.
		  		</p>
		  	</div>
		  </div>
		  <!-- Newest category -->
		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		January 2024
		  	</div>

		  	<div class="col">
		  		<p>
		  			Our paper, <a href="https://arxiv.org/abs/2310.15213">Function Vectors in Large Language Models</a>, was accepted to ICLR 2024!
		  		</p>
		  	</div>
		  </div>
		  <!-- First category -->
		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		May 2023
		  	</div>

		  	<div class="col">
		  		<p>
		  			Our paper, <a href="https://arxiv.org/abs/2305.06299">Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success)</a>, was accepted to ACL 2023!
		  		</p>
		  	</div>
		  </div>

  		  <!-- Second category -->
		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		April 2022
		  	</div>

		  	<div class="col">
		  		<p>
		  			I was awarded a <a href="https://www.nsfgrfp.org/">2022 NSF Graduate Research Fellowship</a>. Northeastern wrote an article about it <a href="https://www.khoury.northeastern.edu/incoming-doctoral-student-millicent-li-wins-national-science-foundation-fellowship/">here</a>.
		  		</p>
		  	</div>
		  </div>

		  <!-- Third category -->
		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		August 2021
		  	</div>

		  	<div class="col">
		  		<p>
		  			Started as an AI Resident with <a href="https://ai.facebook.com/">Fundamental AI Research (FAIR)</a> at Meta in Seattle, working on natural language processing and human-computer interaction research for a year.
		  		</p>
		  	</div>
		  </div>

		  <!-- Fourth category -->
		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		May 2021
		  	</div>

		  	<div class="col">
		  		<p>
		  			Started my internship at <a href="https://www.microsoft.com/en-us/research/">Microsoft Research</a> working with <a href="https://www.microsoft.com/en-us/research/people/tristan/">Tristan Naumann</a> on the intersection of natural language processing and healthcare!
		  		</p>
		  	</div>
		  </div>

		  <!-- Fifth category -->
		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		April 2021
		  	</div>

		  	<div class="col">
		  		<p>
		  			Excited to announce that Iâ€™ll be starting my PhD in the <a href="https://www.khoury.northeastern.edu/">Khoury College of Computer Sciences</a> at <a href="https://www.northeastern.edu/">Northeastern University</a> in Boston, fall of 2022. Thanks to everyone who has supported me on this journey thus far!
		  		</p>
		  	</div>
		  </div>

		  <!-- Sixth category -->
		  <div class="row mx-auto">
		  	<div class="col col-lg-2 yr">
		  		March 2021
		  	</div>

		  	<div class="col">
		  		<p>
		  			I was awarded an Honorable Mention for the <a href="https://www.nsfgrfp.org/">2021 NSF Graduate Research Fellowship</a> competition.
		  		</p>
		  	</div>
		  </div>

		  <!-- Add a divider -->
		  <hr>
		  <!-- Add a divider -->

		  <!-- This is for publications -->
		  <!-- To add a new category, copy paste a row chunk -->
	  	<div class="col">
	  		<h2 class="pubs">Publications</h2>
	  	</div>
		  <div class="publications row mb-3 mx-auto">
		  	<div class="col">
		  		<h3 class="year"><b>2025</b></h3>
		  		<ol class="bibliography2025">
		  			<li>
		  				<div class="row">
		  					<div id="multifield-adaptive-retrieval">
		  						<div class="title">Multi-Field Adaptive Retrieval</div>
		  						<div class="author"><b>Millicent Li</b>, Tongfei Chen, Benjamin Van Durme, Patrick Xia</div>
		  						<div class="periodical">
		  							<em> International Conference on Learning Representations (ICLR), 2025</em>
		  						</div>
		  						<div class="links">
									<button class="btn btn-outline-dark" type="button" data-toggle="collapse" data-target="#collapseExample6" aria-expanded="false" aria-controls="collapseExample6">
										Abstract
									</button>
									<a class="btn btn-outline-dark" href="https://arxiv.org/abs/2410.20056" role="button">HTML</a>
									<div class="collapse" id="collapseExample6">
										<div class="card card-body">
											Document retrieval for tasks such as search and retrieval-augmented generation typically involves datasets that are unstructured: free-form text without explicit internal structure in each document. However, documents can have a structured form, consisting of fields such as an article title, message body, or HTML header. To address this gap, we introduce Multi-Field Adaptive Retrieval (MFAR), a flexible framework that accommodates any number of and any type of document indices on structured data. Our framework consists of two main steps: (1) the decomposition of an existing document into fields, each indexed independently through dense and lexical methods, and (2) learning a model which adaptively predicts the importance of a field by conditioning on the document query, allowing on-the-fly weighting of the most likely field(s). We find that our approach allows for the optimized use of dense versus lexical representations across field types, significantly improves in document ranking over a number of existing retrievers, and achieves state-of-the-art performance for multi-field structured data.
										</div>
									</div>
		  						</div>
		  					</div>
		  				</div>
		  			</li>
		  		</ol>
		  	</div>
		  </div>
	  	<div class="publications row mb-3 mx-auto">
		  	<div class="col">
		  		<h3 class="year"><b>2024</b></h3>
		  		<ol class="bibliography2024">
		  			<li>
		  				<div class="row">
		  					<div class="inner_row" style="margin-bottom: 30px;">
	  							<div id="the-quest">
			  						<div class="title">The Quest for the Right Mediator: A History, Survey, and Theoretical Grounding of Causal Interpretability</div>
			  						<div class="author">Aaron Mueller, ... <b>Millicent Li</b> ... Yonatan Belinkov</div>
			  						<div class="periodical">
			  							<em>ArXiv</em>
			  						</div>
			  						<div class="links">
										<button class="btn btn-outline-dark" type="button" data-toggle="collapse" data-target="#collapseExample5" aria-expanded="false" aria-controls="collapseExample5">
											Abstract
										</button>
										<a class="btn btn-outline-dark" href="https://arxiv.org/abs/2408.01416" role="button">HTML</a>
										<div class="collapse" id="collapseExample5">
											<div class="card card-body">
												Interpretability provides a toolset for understanding how and why neural networks behave in certain ways. However, there is little unity in the field: most studies employ ad-hoc evaluations and do not share theoretical foundations, making it difficult to measure progress and compare the pros and cons of different techniques. Furthermore, while mechanistic understanding is frequently discussed, the basic causal units underlying these mechanisms are often not explicitly defined. In this paper, we propose a perspective on interpretability research grounded in causal mediation analysis. Specifically, we describe the history and current state of interpretability taxonomized according to the types of causal units (mediators) employed, as well as methods used to search over mediators. We discuss the pros and cons of each mediator, providing insights as to when particular kinds of mediators and search methods are most appropriate depending on the goals of a given study. We argue that this framing yields a more cohesive narrative of the field, as well as actionable insights for future work. Specifically, we recommend a focus on discovering new mediators with better trade-offs between human-interpretability and compute-efficiency, and which can uncover more sophisticated abstractions from neural networks than the primarily linear mediators employed in current work. We also argue for more standardized evaluations that enable principled comparisons across mediator types, such that we can better understand when particular causal units are better suited to particular use cases.
											</div>
										</div>
			  						</div>
			  					</div>
			  				</div>
		  				</div>
		  			</li>
		  			<li>
		  				<div class="row">
		  					<div class="inner_row" style="margin-bottom: 30px;">
	  							<div id="function-vectors">
			  						<div class="title">Function Vectors in Large Language Models</div>
			  						<div class="author">Eric Todd, <b>Millicent Li</b>, Arnab Sen Sharma, Aaron Mueller, Byron C Wallace, David Bau</div>
			  						<div class="periodical">
			  							<em> International Conference on Learning Representations (ICLR), 2024</em>
			  						</div>
			  						<div class="links">
										<button class="btn btn-outline-dark" type="button" data-toggle="collapse" data-target="#collapseExample4" aria-expanded="false" aria-controls="collapseExample4">
											Abstract
										</button>
										<a class="btn btn-outline-dark" href="https://arxiv.org/abs/2310.15213" role="button">HTML</a>
										<div class="collapse" id="collapseExample4">
											<div class="card card-body">
												We report the presence of a simple mechanism that represents an input-output function as a vector within autoregressive transformer language models. Using causal mediation analysis on a diverse range of in-context-learning (ICL) tasks, we find that a small number attention heads transport a compact representation of the demonstrated task, which we call a function vector (FV). We test the causal effects of FVs in a variety of input contexts and find that for many tasks FVs are robust to changes in context, i.e., they trigger execution of the task on inputs such as zero-shot and natural text settings that do not resemble ICL. By measuring the causal effects of the FV at each layer of the network, we find that FVs do not directly perform a task through embedding arithmetic, but rather they trigger the model to perform the task using potentially nonlinear computations. Finally, we investigate the internal structure of FVs and find while that they contain information that directly encodes the output space of the function, this information alone is not sufficient to reconstruct an FV. Taken together, our findings suggest that LLMs contain internal abstractions of general-purpose functions that can be invoked in a variety of contexts.
											</div>
										</div>
			  						</div>
			  					</div>
			  				</div>
		  				</div>
		  			</li>
		  		</ol>
		  	</div>
		  </div>
	  	<div class="publications row mb-3 mx-auto">
		  	<div class="col">
		  		<h3 class="year"><b>2023</b></h3>
		  		<ol class="bibliography2023">
		  			<li>
		  				<div class="row">
		  					<div class="inner_row">
			  					<div id="gpt-3-biomed-summ">
			  						<div class="title">Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success)</div>
			  						<div class="author">Chantal Shaib, <b>Millicent L. Li</b>, Sebastian Joseph, Iain Marshall, Junyi Jessy Li, Byron C. Wallace</div>
			  						<div class="periodical">
			  							<em>Annual Meeting of the Association for Computational Linguistics (ACL), 2023</em>
			  						</div>
			  						<div class="links">
										<button class="btn btn-outline-dark" type="button" data-toggle="collapse" data-target="#collapseExample3" aria-expanded="false" aria-controls="collapseExample3">
											Abstract
										</button>
										<a class="btn btn-outline-dark" href="https://arxiv.org/abs/2305.06299" role="button">HTML</a>
										<div class="collapse" id="collapseExample3">
											<div class="card card-body">
												Large language models, particularly GPT-3, are able to produce high quality summaries of general domain news articles in few- and zero-shot settings. However, it is unclear if such models are similarly capable in more specialized, high-stakes domains such as biomedicine. In this paper, we enlist domain experts (individuals with medical training) to evaluate summaries of biomedical articles generated by GPT-3, given zero supervision. We consider both single- and multi-document settings. In the former, GPT-3 is tasked with generating regular and plain-language summaries of articles describing randomized controlled trials; in the latter, we assess the degree to which GPT-3 is able to \emph{synthesize} evidence reported across a collection of articles. We design an annotation scheme for evaluating model outputs, with an emphasis on assessing the factual accuracy of generated summaries. We find that while GPT-3 is able to summarize and simplify single biomedical articles faithfully, it struggles to provide accurate aggregations of findings over multiple documents. We release all data and annotations used in this work.
											</div>
										</div>
			  						</div>
			  					</div>
			  				</div>
		  				</div>
		  			</li>
		  		</ol>
		  	</div>
		  </div>
		  <div class="publications row mb-3 mx-auto">
		  	<div class="col">
		  		<h3 class="year"><b>2022</b></h3>
		  		<ol class="bibliography2022">
		  			<li>
		  				<div class="row">
		  					<div id="review-lm-as-kbs">
		  						<div class="title">A Review on Language Models as Knowledge Bases</div>
		  						<div class="author">Badr AlKhamissi*, <b>Millicent Li*</b>, Asli Celikyilmaz^, Mona Diab^, Marjan Ghazvininejad^</div>
		  						<div class="periodical">
		  							<em>arXiv</em>
		  						</div>
		  						<div class="eq_contrib">
		  							<em>* denotes equal contribution</em>
		  						</div>
		  						<div class="eq_sup">
		  							<em>^ denotes equal supervision</em>
		  						</div>
		  						<div class="links">
									<button class="btn btn-outline-dark" type="button" data-toggle="collapse" data-target="#collapseExample2" aria-expanded="false" aria-controls="collapseExample2">
										Abstract
									</button>
									<a class="btn btn-outline-dark" href="https://arxiv.org/abs/2204.06031" role="button">HTML</a>
									<a class="btn btn-outline-dark" href="https://bkhmsi.github.io/lms-as-kbs/" role="button">Project Website</a>
									<div class="collapse" id="collapseExample2">
										<div class="card card-body">
											Recently, there has been a surge of interest in the NLP community on the use of pretrained Language Models (LMs) as Knowledge Bases (KBs). Researchers have shown that LMs trained on a sufficiently large (web) corpus will encode a significant amount of knowledge implicitly in its parameters. The resulting LM can be probed for different kinds of knowledge and thus acting as a KB. This has a major advantage over traditional KBs in that this method requires no human supervision. In this paper, we present a set of aspects that we deem a LM should have to fully act as a KB, and review the recent literature with respect to those aspects.
										</div>
									</div>
		  						</div>
		  					</div>
		  				</div>
		  			</li>
		  		</ol>
		  	</div>
		  </div>
		  <div class="row mb-3 mx-auto bibliography">
		  	<div class="col">
		  		<h3 class="year"><b>2020</b></h3>
		  		<ol class="bibliography2020">
		  			<li>
		  				<div class="row">
		  					<div id="multichannel-photoplethysmography">
		  						<div class="title">Multi-Channel Facial Photoplethysmography Sensing</div>
		  						<div class="author">Parker S. Ruth, Jerry Cao, <b>Millicent Li</b>, Jacob E. Sunshine, Edward J. Wang, and Shwetak N. Patel</div>
		  						<div class="periodical">
		  							<em>International Conference of the IEEE Engineering in Medicine Biology Society (EMBC 2020)</em>
		  						</div>
		  						<div class="links">
									<button class="btn btn-outline-dark" type="button" data-toggle="collapse" data-target="#collapseExample" aria-expanded="false" aria-controls="collapseExample">
										Abstract
									</button>
									<a class="btn btn-outline-dark" href="https://ieeexplore.ieee.org/document/9176700" role="button">HTML</a>

									<div class="collapse" id="collapseExample">
										<div class="card card-body">
											Motivated by the need for continuous cardiovascular monitoring, we present a system for performing photoplethysmography sensing at multiple facial locations. As a proof-of-concept, our system incorporates an optical sensor array into a wearable face mask form factor for application
											in a surgical hemodynamic monitoring use case. Here we demonstrate that our design can accurately detect pulse timing by validating estimated heart rate against ground truth electrocardiogram recordings. In an experiment across 10 experimental subjects, our system achieves an error standard deviation of 2.84 beats per minute. This system shows promise for
											performing non-invasive, continuous pulse waveform recording from multiple locations on the face.
										</div>
									</div>
		  						</div>
		  					</div>
		  				</div>
		  			</li>
		  		</ol>
		  	</div>
		  </div>

		  <!-- Add a divider -->
		  <hr>
		  <!-- Add a divider -->

	</body>

	<footer>
	    <p class="copyright">Â© 2024 Millicent Li. Last updated on 2025-02-19. </p>
	</footer>
